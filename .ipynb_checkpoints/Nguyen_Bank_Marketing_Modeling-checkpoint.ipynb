{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score,make_scorer, mean_squared_error, mean_absolute_error,\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. DATA Processing\n",
    "\n",
    "https://www.kaggle.com/henriqueyamahata/bank-marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Những chú ý cho quá trình xử lý\n",
    "\n",
    "1. Lược bỏ dữ liệu\n",
    "* Cột 'duration' sẽ không được quan tâm trong quá trình phân tích và xử lý\n",
    "* Có 12 dòng trùng nhau sẽ cắt bỏ\n",
    "2. Missing:\n",
    "* các giá trị unknown của các cột khác: loan, housing, default, marital, job\n",
    "3. Outlier:\n",
    "* Age, campain, previous, cons.conf.idx\n",
    "4. Phân loại lại biến pdays, age\n",
    "5. Clustering theo 3 chỉ số pdays,previous,poutcome\n",
    "6. Sự tương quan giữa các biến:\n",
    "* 3 biến nr.employed, emp.var.rate, euribo3m là 3 biến có tương quan rất mạnh --> có thể lọc bớt để giảm chiều dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cách xử lý\n",
    "1. Xóa 12 dòng dữ liệu trùng nhau\n",
    "2. Loại bỏ cột duration, nr.employed\n",
    "3. Biến 'default': khoảng 20% biến này có giá trị 'unknown' - không xác định đc là client có tín dụng xấu hay không - (yes) hay (no).\n",
    "> Vì đa số biến này cũng khó xác định nên ta sẽ không biến đổi thuộc tính này\n",
    "\n",
    "MISSING\n",
    "4. 2. Biến 'education': 4% là unknown\n",
    "> Thay thế giá trị 'unknown' --> 'university-degree'(bằng giá trị mode của cột dữ liệu)\n",
    "5. Biến 'loan' và 'housing': \n",
    "    * Tỉ lệ missing khá nhỏ: là 2.4% (unknown)\n",
    "    * Cách xử lý: mục tiêu là càng tiếp thị càng nhiều khách hàng càng tốt, ta đánh vào các giá trị mà tỉ lệ sucessful cao:\n",
    "        - loan : unknown --> no\n",
    "        - housing: unknown --> yes\n",
    "6. Biến marital:missing rất thấp 0.1%\n",
    "> unknown --> single\n",
    "7. Biến job: missing thấp 0.8\n",
    "> unknown --> student\n",
    "\n",
    "OUTLIER\n",
    "8. (I). age: thay thế những giá trị > 70 bằng 70\n",
    "Nhìn biểu đồ thấy sau 70 là outlier\n",
    "9. (II). campain: thay thế những giá trị > quantile_95 bằng quantile_95\n",
    "10. (III). previous: thay thế những giá trị > quantile_95 bằng quantile_95\n",
    "11. (IV). cons.conf.idx: thay thế những giá trị > quantile_95 bằng quantile_95\n",
    "\n",
    "Phân loại lại biến\n",
    "12. (I). Column 'pdays'\n",
    "* Có đến 96.3% bộ dữ liệu thuộc tính 'pdays' có giá trị '999' - ghi nhận lại đây là những khách hàng chưa được liên hệ bằng call trước đó.\n",
    "> Vì vậy đối với biến này, ta sẽ biến đổi phân loại thành 3 nhóm: 'not_previously_contacted', 'within_a_week', 'over_a_week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_numerical(df, column, target = None):\n",
    "\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 3));\n",
    "\n",
    "    sns.histplot(df[column], ax=ax1, kde=True);\n",
    "    ax1.set_xlabel(column);\n",
    "    ax1.set_ylabel('Density');\n",
    "    ax1.set_title(f'{column}  Distribution');\n",
    "    \n",
    "    if(target == None):\n",
    "        sns.boxplot(y=column, data=df, showmeans=True, ax=ax2);\n",
    "        ax2.set_ylabel(column);\n",
    "    else:\n",
    "        sns.boxplot(x=target, y=column, data=df, showmeans=True, ax=ax2);\n",
    "        ax2.set_xlabel('Target');\n",
    "        ax2.set_ylabel(column);\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_numerical_lst(df, numerical = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate',\\\n",
    "                            'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], target = None):\n",
    "    for column in numerical:\n",
    "        visualize_numerical(df,column, target)\n",
    "        print();\n",
    "        \n",
    "        \n",
    "def visualize_categorical(df, column, target = 'y'):\n",
    "    \n",
    "        fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,4))    \n",
    "        data1 = df.groupby(column).size()\n",
    "        ax1.pie(x=data1 , autopct=\"%.2f%%\", explode=[0.05]*len(data1), labels=data1.index.tolist(),pctdistance=0.5, radius=1.1)\n",
    "        ax1.set_title(f'{column}  Distribution')\n",
    "\n",
    "        data2 = get_col_target(column, target,df)   \n",
    "        data2.plot(kind='bar',stacked = True, ax=ax2);\n",
    "        plt.xticks(rotation=45);\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "def visualize_categorical_lst(df,categorical = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\\\n",
    "                                          'month', 'day_of_week', 'poutcome'], target = 'y'):\n",
    "    \n",
    "    \n",
    "    for column in categorical:\n",
    "        visualize_categorical(df, column)\n",
    "        \n",
    "def get_col_target(rows, cols,data):\n",
    "    \n",
    "    cols_lst = data[cols].unique().tolist()\n",
    "    rows_lst = data.groupby(rows)[rows].count().sort_values(ascending = False).index.tolist()\n",
    "\n",
    "    group_df = data.groupby([rows,cols]).size()\n",
    "    dic = {}\n",
    "    for item in cols_lst:\n",
    "        vals = []\n",
    "        for i in rows_lst:\n",
    "            try:\n",
    "                vals.append(group_df.loc[(i, item)])\n",
    "            except:\n",
    "                vals.append(0)\n",
    "            finally:\n",
    "                continue\n",
    "        dic[item] = vals\n",
    "\n",
    "    df = pd.DataFrame(dic,index = rows_lst)\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_row(df):\n",
    "    df = df.drop(df[df.duplicated()].index).reset_index(drop=True)\n",
    "    return(df)\n",
    "\n",
    "def remove_features(df,col_lst):\n",
    "    for col in col_lst:\n",
    "        df.pop(col)\n",
    "    return(df)\n",
    "\n",
    "def replace_missing_by_value(df,column,replaced_value,missing_value='unknown'):\n",
    "    df[column] = df[column].apply(lambda val: replaced_value if val == missing_value else val)\n",
    "    return df\n",
    "\n",
    "def replace_outlier_by_quantile(df, column, quantile_thresh = 0.95, replaced_value = None):\n",
    "    thresh_value = df[column].quantile(quantile_thresh)\n",
    "    if (replaced_value == None):\n",
    "        replaced_value = thresh_value\n",
    "        \n",
    "    df[column] = df[column].apply(lambda val: replaced_value if val > thresh_value  else val)\n",
    "    return df\n",
    "\n",
    "def replace_outlier_by_value(df, column, value_thresh, replaced_value = None):\n",
    "    if (replaced_value == None):\n",
    "        replaced_value = value_thresh\n",
    "        \n",
    "    df[column] = df[column].apply(lambda val: replaced_value if val > value_thresh else val)\n",
    "    return df\n",
    "\n",
    "def replace_missing_by_mode(df,column,missing_value='unknown'):\n",
    "    replaced_value = df[column].mode().values.tolist()[0]\n",
    "    df[column] = df[column].apply(lambda val: replaced_value if val == missing_value else val)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_missing_by_median(df,column,missing_value='unknown'):\n",
    "    replaced_value = df[column].median().values.tolist()[0]\n",
    "    df[column] = df[column].apply(lambda val: replaced_value if val == missing_value else val)\n",
    "    return df\n",
    "\n",
    "def transform_pdays(val):\n",
    "    transform_dict = {999:'not_previously_contacted',7: 'over_a_week',0:'within_a_week'}\n",
    "    for key in transform_dict.keys():\n",
    "        if (val >= key):\n",
    "            return transform_dict[key]\n",
    "\n",
    "def eval_class(true, predicted):\n",
    "    acc = metrics.accuracy_score(true, predicted)\n",
    "    precision = metrics.precision_score(true, predicted)\n",
    "    recall = metrics.recall_score(true, predicted)\n",
    "    f1 = metrics.f1_score(true, predicted)\n",
    "    log_loss = metrics.log_loss(true, predicted)\n",
    "    auc = metrics.roc_auc_score(true, predicted)\n",
    "    return acc, precision, recall, f1, log_loss, auc\n",
    "\n",
    "def create_evaluation_df(model_name, y_train,y_train_pred, y_test, y_test_pred):\n",
    "    eval_clm_metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'Log_loss','AUC']\n",
    "    eval_clm_train = [m + '_train' for m in eval_clm_metrics]\n",
    "    eval_clm_test = [m + '_test' for m in eval_clm_metrics]\n",
    "    dis_clm = ['Model','Accuracy_train'] + eval_clm_test + ['diff_Acc_train_test']\n",
    "    dis_clm_1 = ['Model','Accuracy_train','Accuracy_test','Precision_test','Recall_test','F1_test']\n",
    "  \n",
    "    res_clm = pd.DataFrame(data=[[model_name,*eval_class(y_train,y_train_pred),\n",
    "                                 *eval_class(y_test, y_test_pred)]],\n",
    "                          columns=['Model'] + eval_clm_train + eval_clm_test)\n",
    "    res_clm['diff_Acc_train_test'] = res_clm.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)/x.Accuracy_train, axis=1)\n",
    "    return(res_clm[dis_clm_1])\n",
    "\n",
    "def init_evaluation_df():\n",
    "    eval_clm_metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'Log_loss','AUC']\n",
    "    eval_clm_train = [m + '_train' for m in eval_clm_metrics]\n",
    "    eval_clm_test = [m + '_test' for m in eval_clm_metrics]\n",
    "    dis_clm = ['Model','Accuracy_train'] + eval_clm_test + ['diff_Acc_train_test']\n",
    "    dis_clm_1 = ['Model','Accuracy_train','Accuracy_test','Precision_test','Recall_test','F1_test']\n",
    "    \n",
    "    res_clm = pd.DataFrame( columns=['Model'] + eval_clm_train + eval_clm_test + ['diff_Acc_train_test'])\n",
    "\n",
    "    return(res_clm[dis_clm_1])\n",
    "        \n",
    "def data_processing_pipeline(df):\n",
    "    # remove duplicated rows\n",
    "    df = remove_duplicated_row(df)\n",
    "    # remove duration and nr.employed\n",
    "    remove_cols =['duration', 'nr.employed'] \n",
    "    df = remove_features(df,remove_cols)\n",
    "    # edu_unknown = 'unknown'\n",
    "    column = 'education'\n",
    "    replaced_value = df[column].mode().values.tolist()[0]\n",
    "    df = replace_missing_by_value(df,column,replaced_value)\n",
    "\n",
    "    # housing_unknown = 'unknown'\n",
    "    column = 'housing'\n",
    "    replaced_value  = 'yes' #df[column].mode().values.tolist()[0]\n",
    "    df = replace_missing_by_value(df,column,replaced_value)\n",
    "\n",
    "    # loan_unknown = 'unknown'\n",
    "    column = 'loan'\n",
    "    replaced_value  = 'no' # df[column].mode().values.tolist()[0]\n",
    "    df = replace_missing_by_value(df,column,replaced_value)\n",
    "\n",
    "    # marital_unknown = 'unknown'\n",
    "    column = 'marital'\n",
    "    replaced_value  = 'single' # df[column].mode().values.tolist()[0]\n",
    "    df = replace_missing_by_value(df,column,replaced_value)\n",
    "\n",
    "    # job_unknown = 'unknown'\n",
    "    column = 'job'\n",
    "    replaced_value  = 'student' # df[column].mode().values.tolist()[0]\n",
    "    df = replace_missing_by_value(df,column,replaced_value)\n",
    "\n",
    "    ## OUTlier\n",
    "    # age\n",
    "    value_thresh = 65\n",
    "    column = 'age' \n",
    "    df = replace_outlier_by_value(df,column,value_thresh)\n",
    "\n",
    "    # campain\n",
    "    value_thresh = 6\n",
    "    column = 'campaign' \n",
    "    df = replace_outlier_by_value(df,column,value_thresh)\n",
    "\n",
    "    #previous\n",
    "    remove_thresh = float(0.95)\n",
    "    column = 'previous' \n",
    "    df = replace_outlier_by_quantile(df,column)\n",
    "\n",
    "    #cons.conf.idx'\n",
    "    remove_thresh = float(0.95)\n",
    "    column = 'cons.conf.idx' \n",
    "    df = replace_outlier_by_quantile(df,column)\n",
    "\n",
    "    ### PHÂN LOẠI LẠI BIẾN\n",
    "    # pdays\n",
    "    column = 'pdays'\n",
    "    df[column] = df[column].map(transform_pdays)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def label_encode_pipeline(df, cat_col_lst):\n",
    "    labelencoder = LabelEncoder()\n",
    "    for column in cat_col_lst:\n",
    "        df[column] = labelencoder.fit_transform(process_mkt_df[column])\n",
    "    return(df)\n",
    "\n",
    "def run_model(name,model, X_train, y_train, X_test, y_test):\n",
    "    # model_eval_df : evaluation dataframe of model\n",
    "    # y_test_pred_proba: kiểu np.array - dùng để vẽ roc curve\n",
    "    \n",
    "    model_eval_df = pd.DataFrame() # evaluation dataframe\n",
    "    model.fit(X_train,y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:,1] #Lấy xác suất phần 1 \n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    model_eval_df = create_evaluation_df(name, y_train,y_train_pred, y_test, y_test_pred)\n",
    "      \n",
    "    return(model_eval_df,y_test_pred_proba)    \n",
    "\n",
    "def run_model_lst(name_lst,model_lst, X_train, y_train, X_test, y_test):\n",
    "    evalutation_df = init_evaluation_df() # evaluation dataframe\n",
    "    y_test_proba_df = pd.DataFrame() # y_test_proba for ROC curve\n",
    "\n",
    "    for model,name in zip(model_lst,name_lst):\n",
    "        model_eval_df,y_test_pred_proba = run_model(name,model, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        evalutation_df = evalutation_df.append(model_eval_df, ignore_index = True)\n",
    "        y_test_proba_df[name] = y_test_pred_proba\n",
    "    \n",
    "    return(evalutation_df,y_test_proba_df)\n",
    "\n",
    "### ROC CURVE\n",
    "\n",
    "def visualize_ROC_curves(y_true,y_pred_proba_df):\n",
    "    plt.figure(figsize = (15,6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    # Generate ROC curve values: fpr, tpr, thresholds\n",
    "    for col in y_pred_proba_df.columns:\n",
    "        fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, y_pred_proba_df[col])\n",
    "        plt.plot(fpr1, tpr1)\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve: Successful Client Classifiers')\n",
    "    plt.legend(['Base line']+ y_pred_proba_df.columns.tolist(), loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "### Calculate ROI\n",
    "def calculate_roi(call_cnt, sale_cnt,cost_per_call, roi_per_success):\n",
    "    return roi_per_success * sale_cnt  - cost_per_call * call_cnt\n",
    "\n",
    "def get_real_roi(y_test, cost_per_call = 10, roi_per_success = 20):\n",
    "    sale_cnt = (y_test == 1).sum()\n",
    "    call_cnt = len(y_test)\n",
    "    real_roi = calculate_roi(call_cnt, sale_cnt, cost_per_call, roi_per_success)\n",
    "    return real_roi\n",
    "    \n",
    "def get_pred_roi(y_test, y_test_pred,cost_per_call = 10, roi_per_success = 20):\n",
    "    sale_cnt = ((y_test == 1) & (y_test_pred == 1)).sum()\n",
    "    call_cnt = sum((y_test_pred == 1))\n",
    "    pred_roi = calculate_roi(call_cnt, sale_cnt, cost_per_call, roi_per_success)\n",
    "    return pred_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### L O A D Data\n",
    "file_path = \"data/bank-additional-full.csv\"\n",
    "marketing_df = pd.read_csv(file_path,sep = \";\")\n",
    "# marketing_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### M A I N \n",
    "process_mkt_df = marketing_df.copy() \n",
    "test_size = 0.3\n",
    "\n",
    "## Processing data\n",
    "process_mkt_df = data_processing_pipeline(process_mkt_df)\n",
    "cat_cols = process_mkt_df.dtypes[process_mkt_df.dtypes == 'object'].index\n",
    "num_cols = process_mkt_df.dtypes[process_mkt_df.dtypes != 'object'].index\n",
    "\n",
    "## label encoding\n",
    "process_mkt_df = label_encode_pipeline(process_mkt_df, cat_cols)\n",
    "\n",
    "## list of models\n",
    "models = [LogisticRegression(max_iter = 300),\n",
    "#           GaussianNB(),\n",
    "          DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0),\n",
    "#           RandomForestClassifier(n_estimators=1000, max_depth=3),\n",
    "          GradientBoostingClassifier(n_estimators=1000, learning_rate=0.05),\n",
    "          XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder = False)\n",
    "          ]\n",
    "names = [ 'Logistic Regressor',\n",
    "#              'Naive Bayes',\n",
    "            'Decision Tree Classifier',\n",
    "#           'Random Forest Classifier',\n",
    "          'Gradient Boost Classifier',\n",
    "          'XGBoost Classifier'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R U N with the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:42:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#### R U N with the whole data set\n",
    "\n",
    "## split train set and test_set\n",
    "X_train, X_test, y_train, y_test = train_test_split(process_mkt_df.drop('y',axis=1), process_mkt_df['y'],\n",
    "                                                    test_size=test_size, random_state = 101)\n",
    "## standardize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## init data frame of evalutation and y_pred_proba\n",
    "evalutation_df = pd.DataFrame() # evaluation dataframe\n",
    "y_test_pred_proba_df = pd.DataFrame() # y_test_proba for ROC curve\n",
    "\n",
    "## run list of Models to choose the optimal model\n",
    "evalutation_df,y_test_pred_proba_df = run_model_lst(names, models, X_train, y_train, X_test, y_test)\n",
    "# evalutation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R U N with the resampled data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of no clients:  0.5\n",
      "Percentage of yes clients:  0.5\n",
      "Total number of clients in resampled data:  9278\n",
      "\n",
      "Number transactions train dataset:  6494\n",
      "Number transactions test dataset:  2784\n",
      "Total number of transactions:  9278\n"
     ]
    }
   ],
   "source": [
    "def under_resample_data(data, target = 'y'):\n",
    "    X = data.iloc[:, data.columns != target]\n",
    "    y = data.iloc[:, data.columns == target]\n",
    "    # Number of data points in the minority class\n",
    "    number_records_yes = len(data[data.y == 1])\n",
    "    yes_indices = np.array(data[data.y == 1].index)\n",
    "\n",
    "    # Picking the indices of the normal classes\n",
    "    no_indices = data[data.y == 0].index\n",
    "\n",
    "    # Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "    random_no_indices = np.random.choice(no_indices, number_records_yes, replace = False)\n",
    "    random_no_indices = np.array(random_no_indices)\n",
    "\n",
    "    # Appending the 2 indices\n",
    "    under_sample_indices = np.concatenate([yes_indices,random_no_indices])\n",
    "\n",
    "    # Under sample dataset\n",
    "    under_sample_data = data.iloc[under_sample_indices,:]\n",
    "    \n",
    "    return( under_sample_data)\n",
    "\n",
    "#### R U N with the resampled data set\n",
    "\n",
    "## xử lý imbalanced\n",
    "data = process_mkt_df.copy()\n",
    "target = 'y'\n",
    "under_data = under_resample_data(data)\n",
    "\n",
    "## Split train and test undersampled dataset\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(under_data.drop(target,axis=1),under_data[target],\n",
    "                                                                            test_size=test_size, random_state = 101)\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of no clients: \", len(under_data[under_data[target] == 0])/len(under_data))\n",
    "print(\"Percentage of yes clients: \", len(under_data[under_data[target] == 1])/len(under_data))\n",
    "print(\"Total number of clients in resampled data: \", len(under_data))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_under_train))\n",
    "print(\"Number transactions test dataset: \", len(X_under_test))\n",
    "print(\"Total number of transactions: \", len(X_under_train)+len(X_under_test))\n",
    "\n",
    "## Standardize data\n",
    "scaler = MinMaxScaler()\n",
    "X_under_train = scaler.fit_transform(X_under_train)\n",
    "X_under_test = scaler.transform(X_under_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:19:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Precision_test</th>\n",
       "      <th>Recall_test</th>\n",
       "      <th>F1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regressorwith under resampled data</td>\n",
       "      <td>0.728981</td>\n",
       "      <td>0.720546</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.656115</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifierwith under resampled data</td>\n",
       "      <td>0.739914</td>\n",
       "      <td>0.723420</td>\n",
       "      <td>0.769565</td>\n",
       "      <td>0.636691</td>\n",
       "      <td>0.696850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boost Classifierwith under resampled ...</td>\n",
       "      <td>0.785032</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.793778</td>\n",
       "      <td>0.642446</td>\n",
       "      <td>0.710139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost Classifierwith under resampled data</td>\n",
       "      <td>0.927626</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.744472</td>\n",
       "      <td>0.653957</td>\n",
       "      <td>0.696285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy_train  \\\n",
       "0        Logistic Regressorwith under resampled data        0.728981   \n",
       "1  Decision Tree Classifierwith under resampled data        0.739914   \n",
       "2  Gradient Boost Classifierwith under resampled ...        0.785032   \n",
       "3        XGBoost Classifierwith under resampled data        0.927626   \n",
       "\n",
       "   Accuracy_test  Precision_test  Recall_test   F1_test  \n",
       "0       0.720546        0.752475     0.656115  0.700999  \n",
       "1       0.723420        0.769565     0.636691  0.696850  \n",
       "2       0.738147        0.793778     0.642446  0.710139  \n",
       "3       0.715158        0.744472     0.653957  0.696285  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run model list\n",
    "suffix = ' with under resampled data'\n",
    "under_names = [name + suffix for name in names ]\n",
    "## init data frame of evalutation and y_pred_proba\n",
    "under_evalutation_df = pd.DataFrame() # evaluation dataframe\n",
    "y_under_test_pred_proba_df = pd.DataFrame() # y_test_proba for ROC curve\n",
    "\n",
    "under_evalutation_df,y_under_test_pred_proba_df= run_model_lst(under_names, models, X_under_train, y_under_train\n",
    "                                                               , X_under_test, y_under_test)\n",
    "\n",
    "# under_evalutation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_train</th>\n",
       "      <th>Accuracy_test</th>\n",
       "      <th>Precision_test</th>\n",
       "      <th>Recall_test</th>\n",
       "      <th>F1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regressor</td>\n",
       "      <td>0.900219</td>\n",
       "      <td>0.900348</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>0.224576</td>\n",
       "      <td>0.340653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.900045</td>\n",
       "      <td>0.898405</td>\n",
       "      <td>0.704835</td>\n",
       "      <td>0.195621</td>\n",
       "      <td>0.306247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boost Classifier</td>\n",
       "      <td>0.914374</td>\n",
       "      <td>0.897758</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.276130</td>\n",
       "      <td>0.382396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>0.942303</td>\n",
       "      <td>0.893224</td>\n",
       "      <td>0.567268</td>\n",
       "      <td>0.288842</td>\n",
       "      <td>0.382780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regressorwith under resampled data</td>\n",
       "      <td>0.728981</td>\n",
       "      <td>0.720546</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.656115</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Classifierwith under resampled data</td>\n",
       "      <td>0.739914</td>\n",
       "      <td>0.723420</td>\n",
       "      <td>0.769565</td>\n",
       "      <td>0.636691</td>\n",
       "      <td>0.696850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boost Classifierwith under resampled ...</td>\n",
       "      <td>0.785032</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.793778</td>\n",
       "      <td>0.642446</td>\n",
       "      <td>0.710139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost Classifierwith under resampled data</td>\n",
       "      <td>0.927626</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.744472</td>\n",
       "      <td>0.653957</td>\n",
       "      <td>0.696285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy_train  \\\n",
       "0                                 Logistic Regressor        0.900219   \n",
       "1                           Decision Tree Classifier        0.900045   \n",
       "2                          Gradient Boost Classifier        0.914374   \n",
       "3                                 XGBoost Classifier        0.942303   \n",
       "4        Logistic Regressorwith under resampled data        0.728981   \n",
       "5  Decision Tree Classifierwith under resampled data        0.739914   \n",
       "6  Gradient Boost Classifierwith under resampled ...        0.785032   \n",
       "7        XGBoost Classifierwith under resampled data        0.927626   \n",
       "\n",
       "   Accuracy_test  Precision_test  Recall_test   F1_test  \n",
       "0       0.900348        0.705100     0.224576  0.340653  \n",
       "1       0.898405        0.704835     0.195621  0.306247  \n",
       "2       0.897758        0.621622     0.276130  0.382396  \n",
       "3       0.893224        0.567268     0.288842  0.382780  \n",
       "4       0.720546        0.752475     0.656115  0.700999  \n",
       "5       0.723420        0.769565     0.636691  0.696850  \n",
       "6       0.738147        0.793778     0.642446  0.710139  \n",
       "7       0.715158        0.744472     0.653957  0.696285  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.DataFrame()\n",
    "evaluations = evalutation_df.append(under_evalutation_df, ignore_index = True)\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve for the group of models with undersampled data\n",
    "1. Mặc dù accuracy score chạy trên toàn tập data cao hơn, Kết quả chạy với under sample data set lại cho độ Precision, Recall, F1 score tốt hơn và accuracy score trên tập train và test của undersampled data cho kết quả khá khả quan (>70%).\n",
    "--> Sẽ chọn cách undersample data (Mô hình còn có thể cải thiện kết quả tiếp nếu ta cross-validaion, điều này sẽ được cải tiến ở phase sau)\n",
    "\n",
    "2. Vẽ đường ROC curve cho các trường hợp train model trên tập undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "for col in y_test_pred_proba_df.columns:\n",
    "    fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test_undersample, y_test_pred_proba_df[col])\n",
    "    plt.plot(fpr1, tpr1)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_retest_pred_proba_df1)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Successful Client Classifiers')\n",
    "plt.legend(['Base line']+ y_test_pred_proba_df.columns.tolist()+['Gradient_Raw'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "plt.figure(figsize = (15,6))\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "for col in y_test_pred_proba_df.columns:\n",
    "    precision1, recall1, thresholds = precision_recall_curve(y_test_undersample, y_test_pred_proba_df[col])\n",
    "    plt.plot(recall1,precision1)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_retest_pred_proba_df1)\n",
    "plt.plot(recall, precision)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall example')\n",
    "plt.legend(y_test_pred_proba_df.columns.tolist()+['Gradient_Raw'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user imbalanced-learn\n",
    "# Thử over_sample\n",
    "# import numpy as np\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# ros = RandomOverSampler(random_state=123)\n",
    "# X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "# re_eval_df,y_retest_pred_proba_df = run_model(names[1], models[1], X_train_resampled, y_train_resampled, X_test, y_test)\n",
    "# print(re_eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử under_sample\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=123)\n",
    "# X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train, y_train)\n",
    "# un_eval_df,y_untest_pred_proba_df = run_model(names[1], models[1], X_train_undersampled, y_train_undersampled, X_test, y_test)\n",
    "# print(un_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client: 12353\n",
      "real_roi: -95210\n",
      "pred_roi: 970\n",
      "ROI: 96180\n"
     ]
    }
   ],
   "source": [
    "### Real R O I trên tập Test\n",
    "#ROI = 20 * # of sales - 10 * # of calls\n",
    "\n",
    "number_client = len(y_test)\n",
    "real_roi = get_real_roi(y_test)\n",
    "pred_roi = get_pred_roi(y_test, y_test_pred)\n",
    "\n",
    "print('Number of client: '+str(number_client))\n",
    "print('real_roi: '+str(real_roi))\n",
    "print('pred_roi: '+str(pred_roi))\n",
    "print('ROI: '+str(pred_roi - real_roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing           0.031326\n",
       "education         0.034511\n",
       "job               0.035344\n",
       "age               0.035990\n",
       "campaign          0.036638\n",
       "marital           0.037144\n",
       "emp.var.rate      0.038015\n",
       "loan              0.038247\n",
       "day_of_week       0.041355\n",
       "previous          0.042591\n",
       "cons.price.idx    0.043687\n",
       "default           0.050095\n",
       "contact           0.065515\n",
       "cons.conf.idx     0.070622\n",
       "poutcome          0.079831\n",
       "month             0.080324\n",
       "euribor3m         0.117956\n",
       "pdays             0.120809\n",
       "Name: Gradient Boost CLF, dtype: float32"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hệ số mô hình của gradient\n",
    "col_lst = [i for i in process_mkt_df.columns.values.tolist() if i!= 'y']\n",
    "\n",
    "optimal_feature_importance = pd.Series(data = optimal_model.feature_importances_, index = col_lst, name = 'Gradient Boost CLF')\n",
    "optimal_feature_importance.sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
